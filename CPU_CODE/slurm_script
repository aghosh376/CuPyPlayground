#!/bin/bash
#SBATCH --job-name hopBnch
#SBATCH -o jobname_SLURM.out
#SBATCH --partition=short
#SBATCH --time=06:00:00
# vvv sets OMP_NUM_THREADS: 56, 28, 14
#SBATCH --cpus-per-task=56
#SBATCH --nodes=1
#SBATCH --mem=250G
#SBATCH --mail-type END,FAIL
# change this vvv
 ##SBATCH --mail-user xxx@ucmerced.edu
#vv no restart after node failure
#SBATCH --no-requeue

# hrl: from pinnaclesrc
export MKL_THREADING_LAYER=GNU # avoid omp bug
export PYTHONPATH="/home/larsson/bin/pyscf_24/pyscf/pyscf/lib/:$PYTHONPATH"
export PYTHONPATH="/home/larsson/bin/pyscf_24/pyscf/:$PYTHONPATH"
export PYSCF_EXT_PATH="/home/larsson/bin/pyscf_24/pyscf_ext_modules"
export PYTHONPATH="/home/larsson/python/pyutil:$PYTHONPATH"
export PYTHONPATH="/home/larsson/python/lagom:$PYTHONPATH"
#  hrl old_way
conda deactivate 
module purge
. /home/larsson/bin/spack/share/spack/setup-env.sh
source /home/larsson/.load_modules_OLD


thisDir=`pwd`
SCRATCHDIR="/localscratch/${USER}/${SLURM_JOBID}_${SLURM_JOB_NAME}"
export PYSCF_TMPDIR=$SCRATCHDIR
mkdir -p $SCRATCHDIR


echo "===================================================="
echo "        Job ID is:        $SLURM_JOBID"
echo "        Job name is:      $SLURM_JOB_NAME"
echo "        Hostname is:      "`hostname`
echo "        This dir is:      $thisDir"
echo "        CPUs per Task is: $SLURM_CPUS_PER_TASK"
echo "        loadedmodules:    $LOADEDMODULES"
export MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
echo "===================================================="
early(){
    echo "??????????????????????????"
    echo "?? JOB TERMINATED EARLY ??"
    echo "??????????????????????????"
    rm -rf $SCRATCHDIR
}
trap 'early' 1 2 9 15

echo "removing previous slurm output"
rm -f slurm*.out




export PYTHONUNBUFFERED=1
export MKL_THREADING_LAYER=GNU 


module load openmpi/4.1.1-gcc-8.4.1
#module load py-mpi4py-3.1.2-gcc-11.2.0-3ewa5vi

#module load openmpi-4.1.3-gcc-11.2.0-vzxt5ix
#vv this should worrk with ^^ but it does not, surprisingly
module load py-mpi4py-3.1.2-gcc-11.2.0-o4nxnkq

echo "GEN MACHINEFILE"
#MACHINEFILE="nodes.$SLURM_JOB_ID"
MACHINEFILE="NODELIST"
srun -l /bin/hostname | sort -n | awk '{print $2}' > $MACHINEFILE

echo "MPITYPE $SLURM_MPI_TYPE"
echo "TASKS: $SLURM_NTASKS"
echo "MPIRUN"
which mpirun
which srun
echo "LIST:"

srun --mpi=list


echo "===================================================="
echo "= starting at `date`"
echo "===================================================="





mpirun  --map-by ppr:1:node:pe=$SLURM_CPUS_PER_TASK --bind-to none /home/larsson/bin/spack/opt/spack/linux-rhel8-icelake/gcc-11.2.0/python-3.9.12-7y7dl6ojyptcujroc3y2byrflp6ms426/bin/python3 -u hop_benchmark.py >& hop_benchmark.out

rm -rf $SCRATCHDIR


echo "===================================================="
echo "= done at `date`"
echo "===================================================="

